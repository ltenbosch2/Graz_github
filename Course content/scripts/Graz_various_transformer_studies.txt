# in this file, a number of transformer-based trainings are listed.


########## Problem 1: 

# from vector sequence to vector #### 
# the output vector is the mean of a number of input vectors

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import Trainer, TrainingArguments


# ----- 1. Dummy dataset -----
class VectorDataset(Dataset):
    def __init__(self, n_samples=1000, seq_len=10, dim=16):
        # self.X = torch.randn(n_samples, seq_len, dim)
        self.X = (torch.randn(n_samples, seq_len, dim) > 0.5) + 0.0
        # Example: output = mean of inputs
        self.y = self.X.mean(dim=1)
    #
    def __len__(self):
        return len(self.X)
    #
    def __getitem__(self, idx):
        return {"input_vectors": self.X[idx], "labels": self.y[idx]}

# ----- 2. Small transformer model -----
class TransformerRegressor(nn.Module):
    def __init__(self, input_dim=16, model_dim=64, num_heads=4, num_layers=2, output_dim=16):
        super().__init__()
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=model_dim,
            nhead=num_heads,
            dim_feedforward=128,
            dropout=0.1,
            batch_first=True,
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.input_proj = nn.Linear(input_dim, model_dim)
        self.output_proj = nn.Linear(model_dim, output_dim)
    #
    def forward(self, input_vectors, labels=None):
        x = self.input_proj(input_vectors)
        encoded = self.encoder(x)
        pooled = encoded.mean(dim=1)  # simple mean pooling
        preds = self.output_proj(pooled)
        loss = None
        if labels is not None:
            loss_fn = nn.MSELoss()
            loss = loss_fn(preds, labels)
        return {"loss": loss, "logits": preds}

# ----- 3. Trainer-compatible wrappers -----
def collate_fn(batch):
    inputs = torch.stack([item["input_vectors"] for item in batch])
    labels = torch.stack([item["labels"] for item in batch])
    return {"input_vectors": inputs, "labels": labels}

train_dataset = VectorDataset()
val_dataset = VectorDataset(200)

model = TransformerRegressor()

training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=1e-3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=100,
    eval_strategy="epoch", ### or evaluation_strategy # this depends on transformer version
    logging_steps=10,
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=collate_fn,
)

# ----- 4. Train -----
trainer.train()

# 4b ---- plotting -----
import matplotlib.pyplot as plt

# Access log history
logs = trainer.state.log_history

# Extract loss per epoch
train_loss = [entry["loss"] for entry in logs if "loss" in entry]
eval_loss = [entry["eval_loss"] for entry in logs if "eval_loss" in entry]
epochs = range(1, len(eval_loss) + 1)

# Plot
plt.figure(figsize=(6,4))
plt.plot(epochs, eval_loss, label="Eval loss", marker='o')
plt.plot(range(1, len(train_loss) + 1), train_loss, label="Train loss", linestyle='--')
plt.xlabel("Epoch")
plt.ylabel("Loss (MSE)")
plt.title("Training & Evaluation Loss over Epochs")
plt.legend()
plt.grid(True)
plt.savefig('transformers_2.png')
plt.show()

# ----- 5. Test on new case (reals) -----
test_input = torch.randn(1, 10, 16)
with torch.no_grad():
    output = model(test_input)["logits"]

print("Predicted output vector:", output)
test_input.mean(dim=1)

# ----- 5. Test on trained domain (integers) -----
test_input = (torch.randn(1, 10, 16) > 0.5) + 0.0 ### just +0 doesn't work, check
with torch.no_grad():
    output = model(test_input)["logits"]

print("Predicted output vector:", output)
test_input.mean(dim=1)

#>>> print("Predicted output vector:", output)
#Predicted output vector: 
#tensor([[0.3030, 0.3012, 0.5044, 0.4996, 0.4028, 0.1975, 0.4019, 0.3998, 0.1004,
#         0.3003, 0.1008, 0.0991, 0.2001, 0.3034, 0.4035, 0.4023]])
#>>> test_input.mean(dim=1)
#tensor([[0.3000, 0.3000, 0.5000, 0.5000, 0.4000, 0.2000, 0.4000, 0.4000, 0.1000,
#         0.3000, 0.1000, 0.1000, 0.2000, 0.3000, 0.4000, 0.4000]])



####################### from a boolean vector sequence to a particular Boolean expression:

# see class VectorDataset(Dataset) for details about the Boolean expression

# Experiment 2

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from transformers import Trainer, TrainingArguments


# ----- 1. Dummy dataset -----
class VectorDataset(Dataset):
    def __init__(self, n_samples=1000, seq_len=10, dim=3):
        # self.X = torch.randn(n_samples, seq_len, dim)
        X = (torch.randn(n_samples, seq_len, dim) > 0.5) + 0.0
        self.X = X
        y = []
        for i in range(0, n_samples):
             y.append([ 1-X[i][seq_len-1][0], X[i][seq_len-1][1] * X[i][seq_len-2][1], 1-X[i][seq_len-2][2] ])
             #y.append([ X[i][2][0] ])
        #self.y = torch.tensor(y)
        self.y = torch.tensor(y)
        # Example: output = mean of inputs
        # self.y = self.X.mean(dim=1)
    #
    def __len__(self):
        return len(self.X)
    #
    def __getitem__(self, idx):
        return {"input_vectors": self.X[idx], "labels": self.y[idx]}

# test = VectorDataset(1000, 10, 3)
# test.X[33]
# test.y[33]



# ----- 2. Small transformer model -----
class TransformerRegressor(nn.Module):
    def __init__(self, input_dim=3, model_dim=64, num_heads=4, num_layers=4, output_dim=3):
        super().__init__()
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=model_dim,
            nhead=num_heads,
            dim_feedforward=128,
            dropout=0.1,
            batch_first=True,
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.input_proj = nn.Linear(input_dim, model_dim)
        self.output_proj = nn.Linear(model_dim, output_dim)
    #
    def forward(self, input_vectors, labels=None):
        x = self.input_proj(input_vectors)
        encoded = self.encoder(x)
        pooled = encoded.mean(dim=1)  # simple mean pooling
        preds = self.output_proj(pooled)
        loss = None
        if labels is not None:
            loss_fn = nn.MSELoss()
            loss = loss_fn(preds, labels)
        return {"loss": loss, "logits": preds}

# ----- 3. Trainer-compatible wrappers -----
def collate_fn(batch):
    inputs = torch.stack([item["input_vectors"] for item in batch])
    labels = torch.stack([item["labels"] for item in batch])
    return {"input_vectors": inputs, "labels": labels}

train_dataset = VectorDataset(1000, 10, 3)
val_dataset = VectorDataset(200, 10, 3)

model = TransformerRegressor()

training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=1e-3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=1000, # 10,
    eval_strategy="epoch", ### or evaluation_strategy # this depends on transformer version
    logging_steps=10,
    report_to="none",
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=collate_fn,
)

# ----- 4. Train -----
trainer.train()

# ---- 4b. Plotting ----
# Access log history
logs = trainer.state.log_history

# Extract loss per epoch
train_loss = [entry["loss"] for entry in logs if "loss" in entry]
eval_loss = [entry["eval_loss"] for entry in logs if "eval_loss" in entry]
epochs = range(1, len(eval_loss) + 1)

# Plot
plt.figure(figsize=(6,4))
plt.plot(epochs, eval_loss, label="Eval loss", marker='o')
plt.plot(range(1, len(train_loss) + 1), train_loss, label="Train loss", linestyle='--')
plt.xlabel("Epoch")
plt.ylabel("Loss (MSE)")
plt.title("Training & Evaluation Loss over Epochs")
plt.legend()
plt.grid(True)
plt.savefig('transformers_3.png')
plt.show()



# ----- 5. Predict -----

# on test set
test_dataset = VectorDataset(50, 10, 3)
for k in range(0, 10):
  test_input = test_dataset[k]['input_vectors']
  test_input = test_input.unsqueeze(0) # puts an axis in front
  with torch.no_grad():
      output = model(test_input)["logits"]
  #
  print("Predicted: ", output)
  print("Reference: ", test_dataset[k]['labels'])
  print("--------")  ### reasonable performance but certainly not very good

# on training set
for k in range(0, 10):
  test_input = train_dataset[k]['input_vectors']
  test_input = test_input.unsqueeze(0) # puts an axis in front
  with torch.no_grad():
      output = model(test_input)["logits"]
  #
  print("Predicted: ", output)
  print("Reference: ", train_dataset[k]['labels'])
  print("--------")  # quite convincing 



########################## other mappings from vector sequence to boolean:

# Training 3: with a slightly more complicated boolean

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import numpy as np
from sklearn.model_selection import train_test_split

# ----------------------------
# 1. Dataset definition
# ----------------------------
class BooleanSequenceDataset(Dataset):
    def __init__(self, n_samples=2000, seq_len=10):
        self.X, self.y = self._generate_data(n_samples, seq_len)
  #
    def _generate_data(self, n_samples, seq_len):
        # Input: binary sequences of shape [n_samples, seq_len, 2]
        X = torch.randint(0, 2, (n_samples, seq_len, 2)).float()
   #
        # Example Boolean expressions for outputs:
        #   y1 = 1 if any bit in the sequence is 1 (OR over all)
        #   y2 = 1 if the number of 1's in first component > number in second component
        y1 = (X.sum(dim=(1, 2)) > 0).float()  # any 1 in the sequence
        y2 = (X[:, :, 0].sum(dim=1) > X[:, :, 1].sum(dim=1)).float()
        y = torch.stack([y1, y2], dim=1).unsqueeze(-1)  # shape [N, 2, 1]
        return X, y
  #
    def __len__(self):
        return len(self.X)
  #
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# ----------------------------
# 2. Transformer model
# ----------------------------
class BooleanTransformer(nn.Module):
    def __init__(self, input_dim=2, model_dim=32, num_heads=4, num_layers=2, output_dim=2):
        super().__init__()
        self.input_proj = nn.Linear(input_dim, model_dim)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=model_dim,
            nhead=num_heads,
            dim_feedforward=64,
            dropout=0.1,
            batch_first=True,
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        # Pool across sequence dimension
        self.pool = nn.AdaptiveAvgPool1d(1)
        self.output_proj = nn.Sequential(
            nn.Linear(model_dim, 16),
            nn.ReLU(),
            nn.Linear(16, output_dim),
            nn.Sigmoid(),  # because outputs are in [0,1]
        )
    def forward(self, x):
        # x: [batch, seq_len, input_dim]
        x = self.input_proj(x)
        encoded = self.encoder(x)  # [batch, seq_len, model_dim]
        pooled = encoded.mean(dim=1)  # average pooling over sequence
        out = self.output_proj(pooled)  # [batch, output_dim]
        return out.unsqueeze(-1)  # [batch, output_dim, 1]

# ----------------------------
# 3. Training setup
# ----------------------------
def train_model(model, train_loader, val_loader, n_epochs=20, lr=1e-3, device='cpu'):
    criterion = nn.BCELoss()  # binary classification
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    model.to(device)
    for epoch in range(n_epochs):
        model.train()
        running_loss = 0.0
        for X, y in train_loader:
            X, y = X.to(device), y.to(device)
            optimizer.zero_grad()
            y_pred = model(X)
            loss = criterion(y_pred, y)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * X.size(0)
        train_loss = running_loss / len(train_loader.dataset)
        # Validation
        model.eval()
        with torch.no_grad():
            val_loss = 0.0
            for X, y in val_loader:
                X, y = X.to(device), y.to(device)
                val_loss += criterion(model(X), y).item() * X.size(0)
            val_loss /= len(val_loader.dataset)
        print(f"Epoch {epoch+1:02d}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}")

# ----------------------------
# 4. Train and test
# ----------------------------
#def main():
if 1:
    seq_len = 10
    dataset = BooleanSequenceDataset(n_samples=2000, seq_len=seq_len)
    X_train, X_val, y_train, y_val = train_test_split(dataset.X, dataset.y, test_size=0.2)
    #
    train_ds = torch.utils.data.TensorDataset(X_train, y_train)
    val_ds = torch.utils.data.TensorDataset(X_val, y_val)
    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_ds, batch_size=32)
    #
    model = BooleanTransformer()
    train_model(model, train_loader, val_loader, n_epochs=20, lr=1e-3)

if 1:
    # Test
    #testset = BooleanSequenceDataset(n_samples=20, seq_len=seq_len)
    # model.eval()
    X_test = torch.randint(0, 2, (1, seq_len, 2)).float()
    with torch.no_grad():
        #X_test = testset.X[3]
        y_pred = model(X_test)
        print(X_test)
        #print(y_pred)
        print("Sample predictions:\n", y_pred.squeeze(-1).round()) # 
    # first coordinate: any 1 in first coordinate # this works??
    # second coordinate: numer of 1's in first coordinate > number of 1's in second coordinate




# if __name__ == "__main__":
# main()




################ Other experiment: detection of patterns in letter sequences:

# the first function below creates the data set -- various versions


import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import string
import random
import matplotlib.pyplot as plt


# ============================================================
# 1. Synthetic dataset: letter sequences with rule-based labels
# ============================================================
def contains_aba(seq: str) -> int:
    """Return 1 if 'b' is between two 'a's, else 0."""
    for i in range(1, len(seq) - 1):
        #if seq[i] == 'b' and seq[i - 1] == 'a' and seq[i + 1] == 'a':
        #if seq[i] < 'o' and seq[i - 1] == 'f' and seq[i + 1] > 'f':
        #if seq[i] <= 'z' and seq[i - 1] == 'f' and seq[i + 1] >= 'a':
        if seq[i] <= 'k' and 'k'< seq[i - 1] and seq[i-1] < 'r' and 'r' <= seq[i + 1]:
            return 1
    return 0

def generate_dataset(n_samples=5000, seq_len=8):
    alphabet = list(string.ascii_lowercase)
    sequences, labels = [], []
    for _ in range(n_samples):
        seq = ''.join(random.choices(alphabet, k=seq_len))
        sequences.append(seq)
        labels.append(contains_aba(seq))
    return sequences, np.array(labels, dtype=float)


# ============================================================
# 2. Character encoding utilities
# ============================================================
class CharTokenizer:
    def __init__(self):
        self.vocab = list(string.ascii_lowercase)
        self.char2idx = {c: i for i, c in enumerate(self.vocab)}
        self.vocab_size = len(self.vocab)
    #
    def encode(self, seq):
        return [self.char2idx[c] for c in seq]
    #
    def encode_batch(self, seqs):
        return torch.tensor([self.encode(s) for s in seqs], dtype=torch.long)


# ============================================================
# 3. Transformer Model for sequence → scalar
# ============================================================
class TransformerLetterClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim=32, model_dim=64,
                 num_heads=4, num_layers=2, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.input_fc = nn.Linear(embed_dim, model_dim)
        #
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=model_dim,
            nhead=num_heads,
            dim_feedforward=model_dim * 4,
            dropout=dropout,
            activation='relu',
            batch_first=True
        )
        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.output_fc = nn.Linear(model_dim, 1)
    #
    def forward(self, x):
        # x: (batch, seq_len)
        x = self.embedding(x)           # (batch, seq_len, embed_dim)
        x = self.input_fc(x)            # (batch, seq_len, model_dim)
        x = self.transformer(x)         # (batch, seq_len, model_dim)
        pooled = x.mean(dim=1)          # average across sequence positions
        out = self.output_fc(pooled).squeeze(-1)
        return torch.sigmoid(out)       # binary classification (0–1)


# ============================================================
# 4. Training and Evaluation
# ============================================================
def train_model(model, train_loader, val_loader, num_epochs=20, lr=1e-3, device='cpu'):
    criterion = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    model.to(device)
    #
    train_losses, val_losses = [], []
    for epoch in range(num_epochs):
        model.train()
        total_train_loss = 0
        for X_batch, y_batch in train_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item() * X_batch.size(0)
        avg_train_loss = total_train_loss / len(train_loader.dataset)
        train_losses.append(avg_train_loss)
        #
        # validation
        model.eval()
        total_val_loss = 0
        with torch.no_grad():
            for X_batch, y_batch in val_loader:
                X_batch, y_batch = X_batch.to(device), y_batch.to(device)
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                total_val_loss += loss.item() * X_batch.size(0)
        avg_val_loss = total_val_loss / len(val_loader.dataset)
        val_losses.append(avg_val_loss)
        #
        print(f"Epoch {epoch+1:02d}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}")
    return train_losses, val_losses


# ============================================================
# 5. Main
# ============================================================
#if __name__ == "__main__":
if 1:
    random.seed(42)
    np.random.seed(42)
    torch.manual_seed(42)
    #
    seq_len = 8
    train_seqs, train_y = generate_dataset(8000, seq_len)
    val_seqs,   val_y   = generate_dataset(1000, seq_len)
    test_seqs,  test_y  = generate_dataset(1000, seq_len)
    #
    tokenizer = CharTokenizer()
    #
    X_train = tokenizer.encode_batch(train_seqs)
    X_val   = tokenizer.encode_batch(val_seqs)
    X_test  = tokenizer.encode_batch(test_seqs)
    #
    y_train = torch.tensor(train_y, dtype=torch.float32)
    y_val   = torch.tensor(val_y, dtype=torch.float32)
    y_test  = torch.tensor(test_y, dtype=torch.float32)
    #
    batch_size = 64
    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)
    val_loader   = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)
    test_loader  = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)
    #
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    #
    model = TransformerLetterClassifier(vocab_size=tokenizer.vocab_size)
    train_losses, val_losses = train_model(model, train_loader, val_loader,
                                           num_epochs=250, lr=1e-3, device=device)


if 1:
    # plot losses
    plt.figure(figsize=(7,4))
    plt.plot(train_losses, label="Train Loss")
    plt.plot(val_losses, label="Val Loss")
    plt.xlabel("Epoch"); plt.ylabel("Loss")
    plt.legend(); plt.grid(True)
    plt.title("Training vs Validation Loss")
    plt.savefig("letter_sequences_aba_v3.png")
    plt.show()

if 1:
    # evaluate on test set
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch = X_batch.to(device)
            outputs = model(X_batch).cpu().numpy()
            preds.append(outputs)
            trues.append(y_batch.numpy())
    preds = np.concatenate(preds)
    trues = np.concatenate(trues)
    acc = ((preds > 0.5).astype(float) == trues).mean()
    print(f"\nTest Accuracy: {acc:.4f}")  
    ### 0.8850, np.sum(train_y)/len(train_y) == 0.0925
    ### 0.9150, np.sum(train_y)/len(train_y) == 0.2068
    ### 0.7190,  np.sum(train_y)/len(train_y) == 0.1975

example (case 2):
preds[33:38]
array([1.1092579e-11, 1.8448012e-11, 9.7479558e-01, 1.1126758e-11,
       1.7761188e-11], dtype=float32)
train_seqs[33:38]
['vspzravh', 'ryddcohp', 'sfqgmxvc', 'lhauqgto', 'labwxovp']